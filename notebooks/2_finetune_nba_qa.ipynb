{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune NBA QA Model\n",
    "\n",
    "This notebook fine-tunes a question-answering model on NBA-specific data.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The fine-tuning process:\n",
    "1. **Load Dataset**: Load the NBA QA dataset generated in the previous notebook\n",
    "2. **Prepare Data**: Split into train/validation sets and tokenize\n",
    "3. **Fine-tune Model**: Train the QA model on NBA-specific questions\n",
    "4. **Evaluate**: Test the fine-tuned model's performance\n",
    "5. **Save Model**: Save the fine-tuned model for use in the QA system\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Dataset generated from `1_generate_nba_qa_dataset.ipynb`\n",
    "- Base model: `deepset/roberta-base-squad2`\n",
    "- GPU recommended for faster training (CPU will work but slower)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install Required Packages\n",
    "\n",
    "Run this cell first to install the required packages if they're not already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:39.975093Z",
     "start_time": "2025-12-09T03:21:38.853919Z"
    }
   },
   "source": [
    "# Install required packages\n",
    "# Suppress PATH warning for scripts location\n",
    "%pip install transformers torch datasets accelerate ipywidgets --no-warn-script-location\n",
    "\n",
    "print(\"âœ… Packages installed successfully!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: transformers in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (4.57.3)\r\n",
      "Requirement already satisfied: torch in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: datasets in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (4.4.1)\r\n",
      "Requirement already satisfied: accelerate in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (1.10.1)\r\n",
      "Requirement already satisfied: ipywidgets in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (8.1.8)\r\n",
      "Requirement already satisfied: filelock in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (3.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (2.0.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (2025.9.1)\r\n",
      "Requirement already satisfied: requests in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (2.32.5)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (21.0.0)\r\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (0.4.0)\r\n",
      "Requirement already satisfied: pandas in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (2.3.2)\r\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (0.28.1)\r\n",
      "Requirement already satisfied: xxhash in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (3.6.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.18)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\r\n",
      "Requirement already satisfied: anyio in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (4.12.0)\r\n",
      "Requirement already satisfied: certifi in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (2025.8.3)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpx<1.0.0->datasets) (3.10)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\r\n",
      "Requirement already satisfied: psutil in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from accelerate) (7.1.3)\r\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (0.2.3)\r\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (8.18.1)\r\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (5.14.3)\r\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (4.0.15)\r\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipywidgets) (3.0.16)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\r\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (4.0.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\r\n",
      "Requirement already satisfied: decorator in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\r\n",
      "Requirement already satisfied: matplotlib-inline in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\r\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\r\n",
      "Requirement already satisfied: stack-data in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\r\n",
      "Requirement already satisfied: exceptiongroup in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4.3)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.20)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\r\n",
      "Requirement already satisfied: pure-eval in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ… Packages installed successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "\n",
    "**Note**: If you get a `ModuleNotFoundError`, make sure you ran the installation cell above.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:44.206124Z",
     "start_time": "2025-12-09T03:21:39.980546Z"
    }
   },
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "from transformers import pipeline\n",
    "\n",
    "# Suppress tqdm IProgress warning\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tqdm')\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='torch.utils.data.dataloader')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "PyTorch version: 2.8.0\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Fine-Tuner Class\n",
    "\n",
    "This class handles:\n",
    "- Loading the NBA QA dataset\n",
    "- Tokenizing examples for training\n",
    "- Fine-tuning the model\n",
    "- Evaluating performance\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:44.226604Z",
     "start_time": "2025-12-09T03:21:44.216271Z"
    }
   },
   "source": [
    "class NBAQAFineTuner:\n",
    "    \"\"\"Fine-tune QA model on NBA dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model: str = \"deepset/roberta-base-squad2\"):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def load_dataset(self, dataset_path: str) -> Dataset:\n",
    "        \"\"\"Load NBA QA dataset from JSON file\"\"\"\n",
    "        print(f\"Loading dataset from {dataset_path}...\")\n",
    "        \n",
    "        with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "            squad_data = json.load(f)\n",
    "        \n",
    "        # Convert SQuAD format to list of examples\n",
    "        examples = []\n",
    "        for paragraph in squad_data[\"data\"][0][\"paragraphs\"]:\n",
    "            context = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question = qa[\"question\"]\n",
    "                answers = qa[\"answers\"]\n",
    "                \n",
    "                if answers and len(answers) > 0:\n",
    "                    answer_text = answers[0][\"text\"]\n",
    "                    answer_start = answers[0][\"answer_start\"]\n",
    "                    \n",
    "                    # Store as list of dicts (SQuAD format)\n",
    "                    examples.append({\n",
    "                        \"context\": context,\n",
    "                        \"question\": question,\n",
    "                        \"answers\": [{\n",
    "                            \"text\": answer_text,\n",
    "                            \"answer_start\": answer_start\n",
    "                        }]\n",
    "                    })\n",
    "        \n",
    "        print(f\"Loaded {len(examples)} examples\")\n",
    "        return Dataset.from_list(examples)\n",
    "    \n",
    "    def prepare_dataset(self, dataset: Dataset, train_split: float = 0.8):\n",
    "        \"\"\"Prepare dataset for training\"\"\"\n",
    "        print(\"Preparing dataset...\")\n",
    "        \n",
    "        # Split into train and validation\n",
    "        dataset = dataset.train_test_split(test_size=1 - train_split, seed=42)\n",
    "        train_dataset = dataset[\"train\"]\n",
    "        val_dataset = dataset[\"test\"]\n",
    "        \n",
    "        print(f\"Train examples: {len(train_dataset)}\")\n",
    "        print(f\"Validation examples: {len(val_dataset)}\")\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "    \n",
    "    def tokenize_function(self, examples: Dict[str, List]) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenize examples for QA task\"\"\"\n",
    "        questions = examples[\"question\"]\n",
    "        contexts = examples[\"context\"]\n",
    "        answers_list = examples[\"answers\"]\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized = self.tokenizer(\n",
    "            questions,\n",
    "            contexts,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=512,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Find answer positions in tokenized sequence\n",
    "        start_positions = []\n",
    "        end_positions = []\n",
    "        \n",
    "        for i in range(len(questions)):\n",
    "            # Extract answer from the nested structure\n",
    "            if isinstance(answers_list[i], list) and len(answers_list[i]) > 0:\n",
    "                answer_dict = answers_list[i][0]\n",
    "                answer_text = answer_dict.get(\"text\", \"\")\n",
    "                answer_start_char = answer_dict.get(\"answer_start\", -1)\n",
    "            elif isinstance(answers_list[i], dict):\n",
    "                answer_text = answers_list[i].get(\"text\", \"\")\n",
    "                answer_start_char = answers_list[i].get(\"answer_start\", -1)\n",
    "            else:\n",
    "                answer_text = \"\"\n",
    "                answer_start_char = -1\n",
    "            \n",
    "            # Find answer in context\n",
    "            if answer_start_char >= 0:\n",
    "                context_start = contexts[i].find(answer_text, max(0, answer_start_char - 10))\n",
    "            else:\n",
    "                context_start = contexts[i].find(answer_text)\n",
    "            \n",
    "            if context_start == -1 or not answer_text:\n",
    "                start_positions.append(-1)\n",
    "                end_positions.append(-1)\n",
    "                continue\n",
    "            \n",
    "            context_end = context_start + len(answer_text)\n",
    "            \n",
    "            # Map character positions to token positions\n",
    "            offset_mapping = tokenized[\"offset_mapping\"][i]\n",
    "            start_token = -1\n",
    "            end_token = -1\n",
    "            \n",
    "            for token_idx, (token_start, token_end) in enumerate(offset_mapping):\n",
    "                if token_start == 0 and token_end == 0:\n",
    "                    continue\n",
    "                    \n",
    "                if token_start <= context_start < token_end and start_token == -1:\n",
    "                    start_token = token_idx\n",
    "                if token_start < context_end <= token_end and end_token == -1:\n",
    "                    end_token = token_idx\n",
    "                    break\n",
    "            \n",
    "            if start_token != -1 and end_token == -1:\n",
    "                end_token = start_token + 1\n",
    "            \n",
    "            start_positions.append(start_token if start_token != -1 else -1)\n",
    "            end_positions.append(end_token if end_token != -1 else -1)\n",
    "        \n",
    "        tokenized[\"start_positions\"] = start_positions\n",
    "        tokenized[\"end_positions\"] = end_positions\n",
    "        tokenized.pop(\"offset_mapping\", None)\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "print(\"âœ… NBAQAFineTuner class defined!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NBAQAFineTuner class defined!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Prepare Dataset\n",
    "\n",
    "Load the dataset generated in the previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:44.261859Z",
     "start_time": "2025-12-09T03:21:44.232529Z"
    }
   },
   "source": [
    "# Configuration\n",
    "dataset_path = \"../utils/nba_qa_dataset.json\"\n",
    "base_model = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"âŒ Dataset not found at {dataset_path}\")\n",
    "    print(\"Please run 1_generate_nba_qa_dataset.ipynb first to generate the dataset.\")\n",
    "else:\n",
    "    # Initialize fine-tuner\n",
    "    fine_tuner = NBAQAFineTuner(base_model=base_model)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = fine_tuner.load_dataset(dataset_path)\n",
    "    \n",
    "    # Prepare train/val split\n",
    "    train_dataset, val_dataset = fine_tuner.prepare_dataset(dataset, train_split=0.8)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from utils/nba_qa_dataset.json...\n",
      "Loaded 1803 examples\n",
      "Preparing dataset...\n",
      "Train examples: 1442\n",
      "Validation examples: 361\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model and Tokenizer\n",
    "\n",
    "Load the base model and tokenizer. This will download the model if not already cached.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:44.911656Z",
     "start_time": "2025-12-09T03:21:44.265970Z"
    }
   },
   "source": [
    "print(\"ðŸ“¥ Loading tokenizer and model...\")\n",
    "fine_tuner.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "fine_tuner.model = AutoModelForQuestionAnswering.from_pretrained(base_model)\n",
    "\n",
    "print(f\"âœ… Model loaded: {base_model}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in fine_tuner.model.parameters()):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading tokenizer and model...\n",
      "âœ… Model loaded: deepset/roberta-base-squad2\n",
      "Model parameters: 124,056,578\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Tokenize Datasets\n",
    "\n",
    "Tokenize the training and validation datasets for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:48.307238Z",
     "start_time": "2025-12-09T03:21:44.915198Z"
    }
   },
   "source": [
    "print(\"ðŸ”¤ Tokenizing datasets...\")\n",
    "train_tokenized = train_dataset.map(\n",
    "    fine_tuner.tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "val_tokenized = val_dataset.map(\n",
    "    fine_tuner.tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "# Filter out examples with invalid answer positions\n",
    "train_tokenized = train_tokenized.filter(lambda x: x[\"start_positions\"] != -1)\n",
    "val_tokenized = val_tokenized.filter(lambda x: x[\"start_positions\"] != -1)\n",
    "\n",
    "print(f\"âœ… Tokenization complete!\")\n",
    "print(f\"Train examples after filtering: {len(train_tokenized)}\")\n",
    "print(f\"Validation examples after filtering: {len(val_tokenized)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¤ Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1442 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "788735335e224962bae6240d7762e738"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/361 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19337c71ffed41159d9de23c28b5f510"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/1442 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b5a1e8fbe1b9424fa5f22ee7892eae42"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Filter:   0%|          | 0/361 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70c7d6885e254d4b8eb08b52dd8784eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenization complete!\n",
      "Train examples after filtering: 1433\n",
      "Validation examples after filtering: 358\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Configure Training Arguments\n",
    "\n",
    "Set up training parameters. Adjust these based on your hardware and requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:21:48.355342Z",
     "start_time": "2025-12-09T03:21:48.317508Z"
    }
   },
   "source": [
    "# Training configuration\n",
    "output_dir = \"../utils/model_nba_qa_finetuned\"\n",
    "num_epochs = 3\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 250\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if GPU available\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Training arguments configured!\")\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Training arguments configured!\n",
      "Output directory: utils/model_nba_qa_finetuned\n",
      "Epochs: 3\n",
      "Batch size: 8\n",
      "Learning rate: 2e-05\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Initialize Trainer and Start Training\n",
    "\n",
    "This will start the fine-tuning process. Training time depends on:\n",
    "- Number of examples\n",
    "- Hardware (GPU vs CPU)\n",
    "- Number of epochs\n",
    "\n",
    "**Note**: This may take 15-30 minutes on GPU, or 2-4 hours on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:56:15.147455Z",
     "start_time": "2025-12-09T03:21:48.365182Z"
    }
   },
   "source": [
    "# Data collator\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=fine_tuner.model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nðŸ‹ï¸ Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "train_result = trainer.train()\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\nâœ… Training complete!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‹ï¸ Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 34:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.162800</td>\n",
       "      <td>0.089596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179600</td>\n",
       "      <td>0.072905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.083300</td>\n",
       "      <td>0.082318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\n",
      "âœ… Training complete!\n",
      "Training loss: 0.2703\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the Fine-tuned Model\n",
    "\n",
    "Save the model and tokenizer for use in the QA system.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:56:17.060220Z",
     "start_time": "2025-12-09T03:56:15.351330Z"
    }
   },
   "source": [
    "print(f\"ðŸ’¾ Saving model to {output_dir}...\")\n",
    "trainer.save_model()\n",
    "fine_tuner.tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"âœ… Model saved to: {output_dir}\")\n",
    "print(\"\\nThe fine-tuned model is ready to use in the QA system!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saving model to utils/model_nba_qa_finetuned...\n",
      "âœ… Model saved to: utils/model_nba_qa_finetuned\n",
      "\n",
      "The fine-tuned model is ready to use in the QA system!\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Evaluate the Model (Optional)\n",
    "\n",
    "Test the fine-tuned model on the validation set to see its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T03:56:37.677605Z",
     "start_time": "2025-12-09T03:56:17.206834Z"
    }
   },
   "source": [
    "# Load fine-tuned model for evaluation\n",
    "print(f\"ðŸ“Š Evaluating model from {output_dir}...\")\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=output_dir,\n",
    "    tokenizer=output_dir\n",
    ")\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for example in val_dataset:\n",
    "    result = qa_pipeline(\n",
    "        question=example[\"question\"],\n",
    "        context=example[\"context\"]\n",
    "    )\n",
    "    \n",
    "    predicted_answer = result[\"answer\"].strip()\n",
    "    \n",
    "    # Extract true answer\n",
    "    answers = example[\"answers\"]\n",
    "    if isinstance(answers, list) and len(answers) > 0:\n",
    "        true_answer = answers[0].get(\"text\", \"\").strip()\n",
    "    else:\n",
    "        true_answer = \"\"\n",
    "    \n",
    "    # Simple exact match\n",
    "    if true_answer and predicted_answer.lower() == true_answer.lower():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"âœ… Evaluation complete!\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Evaluating model from utils/model_nba_qa_finetuned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation complete!\n",
      "Accuracy: 100.00% (361/361)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "âœ… **Fine-tuning complete!**\n",
    "\n",
    "The fine-tuned model is saved at: `utils/model_nba_qa_finetuned/`\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Use the fine-tuned model in the QA system:\n",
    "```python\n",
    "from nba_qa_system import NBAQASystem\n",
    "\n",
    "qa_system = NBAQASystem(model_name=\"utils/model_nba_qa_finetuned\")\n",
    "result = qa_system.answer(\"What is LeBron James' career PPG?\")\n",
    "print(result.answer)\n",
    "```\n",
    "\n",
    "Or continue to the next notebook: `3_nba_qa_system.ipynb`\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
