{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Verifier System\n",
    "\n",
    "This notebook implements an information verification system using:\n",
    "- **LangGraph**: Workflow orchestration\n",
    "- **LangChain**: LLM integration and tools\n",
    "- **Hugging Face**: Classification models\n",
    "\n",
    "## Architecture\n",
    "\n",
    "1. User input → Query enhancement\n",
    "2. Information retrieval (web search + domain APIs)\n",
    "3. Evidence extraction and analysis\n",
    "4. Classification (real/fake/doubtful)\n",
    "5. Explanation generation with sources\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.280189Z",
     "start_time": "2025-11-29T16:54:03.957593Z"
    }
   },
   "source": [
    "%pip install langchain langchain-openai langgraph langchain-community langchain-tavily transformers torch\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: langchain in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (0.3.27)\r\n",
      "Requirement already satisfied: langchain-openai in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (0.3.35)\r\n",
      "Requirement already satisfied: langgraph in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (0.6.11)\r\n",
      "Requirement already satisfied: langchain-community in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (0.3.31)\r\n",
      "Requirement already satisfied: langchain-tavily in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (0.2.11)\r\n",
      "Requirement already satisfied: transformers in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (4.57.3)\r\n",
      "Requirement already satisfied: torch in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (2.8.0)\r\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.80)\r\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (0.3.11)\r\n",
      "Requirement already satisfied: langsmith>=0.1.17 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (0.4.37)\r\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (2.11.7)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (2.0.44)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (2.32.5)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain) (4.0.3)\r\n",
      "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (2.8.1)\r\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-openai) (0.12.0)\r\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph) (2.1.2)\r\n",
      "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.6.5)\r\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph) (0.2.9)\r\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph) (3.6.0)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (3.13.2)\r\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (9.1.2)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.6.7)\r\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.11.0)\r\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (0.4.3)\r\n",
      "Requirement already satisfied: numpy>=1.26.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-community) (2.0.2)\r\n",
      "Requirement already satisfied: filelock in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (3.19.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (25.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (2025.9.1)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.22.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from torch) (2025.9.0)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\r\n",
      "Requirement already satisfied: ormsgpack>=1.10.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.11.0)\r\n",
      "Requirement already satisfied: httpx>=0.25.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\r\n",
      "Requirement already satisfied: orjson>=3.10.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\r\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\r\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.12.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\r\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.12.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\r\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2->langchain) (2025.8.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.104.2->langchain-openai) (1.3.0)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/ralvarez/Library/Python/3.9/lib/python/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.3\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49m/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.291966Z",
     "start_time": "2025-11-29T16:54:05.289058Z"
    }
   },
   "source": [
    "# Suppress warnings (optional - these are harmless but can be noisy)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', message='.*urllib3.*')\n",
    "warnings.filterwarnings('ignore', message='.*tqdm.*')\n",
    "warnings.filterwarnings('ignore', message='.*IProgress.*')\n",
    "\n",
    "# Suppress urllib3 OpenSSL warning specifically\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.NotOpenSSLWarning)\n"
   ],
   "outputs": [],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.302966Z",
     "start_time": "2025-11-29T16:54:05.296953Z"
    }
   },
   "source": [
    "import os\n",
    "from typing import TypedDict, Literal, List, Annotated\n",
    "from pydantic import BaseModel\n",
    "from enum import Enum\n",
    "\n",
    "# Compatible StrEnum for Python < 3.11\n",
    "try:\n",
    "    from enum import StrEnum, auto\n",
    "except ImportError:\n",
    "    # Fallback for Python < 3.11\n",
    "    class StrEnum(str, Enum):\n",
    "        @staticmethod\n",
    "        def _generate_next_value_(name, start, count, last_values):\n",
    "            return name.lower()\n",
    "    \n",
    "    # auto() for Python < 3.11 compatibility\n",
    "    _auto_value = object()\n",
    "    def auto():\n",
    "        return _auto_value\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import StateGraph, START, END, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# Try to import Tavily (new package), fallback to DuckDuckGo if not available\n",
    "TAVILY_SEARCH_CLASS = None\n",
    "try:\n",
    "    # New package - preferred\n",
    "    from langchain_tavily import TavilySearch\n",
    "    TAVILY_SEARCH_CLASS = TavilySearch\n",
    "    TAVILY_AVAILABLE = True\n",
    "    print(\"✅ Using langchain-tavily package (new)\")\n",
    "except ImportError:\n",
    "    # Fallback to old package if new one not available\n",
    "    try:\n",
    "        from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "        TAVILY_SEARCH_CLASS = TavilySearchResults\n",
    "        TAVILY_AVAILABLE = True\n",
    "        print(\"⚠️  Using deprecated langchain-community.tools.tavily_search - consider upgrading to langchain-tavily\")\n",
    "    except ImportError:\n",
    "        try:\n",
    "            from langchain_community.tools import DuckDuckGoSearchRun\n",
    "            TAVILY_AVAILABLE = False\n",
    "            print(\"Tavily not available, will use DuckDuckGo as fallback\")\n",
    "        except ImportError:\n",
    "            TAVILY_AVAILABLE = False\n",
    "            print(\"No search tool available. Please install langchain-tavily or ensure langchain-community is installed.\")\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import pipeline\n",
    "\n",
    "# For domain-specific APIs (example: sports)\n",
    "import requests\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using langchain-tavily package (new)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.318361Z",
     "start_time": "2025-11-29T16:54:05.313937Z"
    }
   },
   "source": [
    "# =============================================================================\n",
    "# SETUP API KEYS - RUN THIS CELL FIRST!\n",
    "# =============================================================================\n",
    "\n",
    "# ⚠️ REQUIRED: OpenAI API Key (you must set this!)\n",
    "# Replace the text below with your actual API key from https://platform.openai.com/api-keys\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key-here\"  # ⬅️ CHANGE THIS to your actual key (starts with \"sk-\")\n",
    "\n",
    "# Optional: Tavily API Key (you already have this set, so this is fine)\n",
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "\n",
    "# Option 2: Load from environment variable (RECOMMENDED)\n",
    "# Set it in your terminal before running Jupyter:\n",
    "# export OPENAI_API_KEY=\"your-key-here\"\n",
    "# export TAVILY_API_KEY=\"your-key-here\"  # Optional\n",
    "\n",
    "# Option 3: Load from a .env file (RECOMMENDED for local development)\n",
    "# First install: pip install python-dotenv\n",
    "# Then create a .env file with:\n",
    "# OPENAI_API_KEY=your-key-here\n",
    "# TAVILY_API_KEY=your-key-here\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded environment variables from .env file\")\n",
    "except ImportError:\n",
    "    print(\"python-dotenv not installed. Install with: pip install python-dotenv\")\n",
    "    print(\"Or set environment variables manually.\")\n",
    "\n",
    "# Check if API key is set\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"⚠️  WARNING: OPENAI_API_KEY not set!\")\n",
    "    print(\"Please set it using one of the methods above.\")\n",
    "    print(\"\\nTo get an OpenAI API key:\")\n",
    "    print(\"1. Go to https://platform.openai.com/api-keys\")\n",
    "    print(\"2. Sign up or log in\")\n",
    "    print(\"3. Create a new API key\")\n",
    "    print(\"4. Copy it and set it using one of the methods above\")\n",
    "else:\n",
    "    print(\"✅ OPENAI_API_KEY is set\")\n",
    "\n",
    "# Tavily is optional - will fallback to DuckDuckGo if not set\n",
    "if os.environ.get(\"TAVILY_API_KEY\"):\n",
    "    print(\"✅ TAVILY_API_KEY is set\")\n",
    "else:\n",
    "    print(\"ℹ️  TAVILY_API_KEY not set - will use DuckDuckGo for search (free, no API key needed)\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment variables from .env file\n",
      "✅ OPENAI_API_KEY is set\n",
      "✅ TAVILY_API_KEY is set\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.330261Z",
     "start_time": "2025-11-29T16:54:05.327658Z"
    }
   },
   "source": [
    "# Verify API key is set before proceeding\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\n",
    "        \"❌ ERROR: OPENAI_API_KEY is not set!\\n\\n\"\n",
    "        \"Please set it in the 'Setup API keys' cell above using one of these methods:\\n\\n\"\n",
    "        \"Method 1 - Direct (quick):\\n\"\n",
    "        \"  os.environ['OPENAI_API_KEY'] = 'sk-your-key-here'\\n\\n\"\n",
    "        \"Method 2 - Environment variable:\\n\"\n",
    "        \"  export OPENAI_API_KEY='sk-your-key-here'\\n\\n\"\n",
    "        \"Method 3 - .env file:\\n\"\n",
    "        \"  Create a .env file with: OPENAI_API_KEY=sk-your-key-here\\n\\n\"\n",
    "        \"Get your API key from: https://platform.openai.com/api-keys\"\n",
    "    )\n",
    "else:\n",
    "    print(\"✅ API key verified - ready to initialize model\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key verified - ready to initialize model\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:05.345972Z",
     "start_time": "2025-11-29T16:54:05.340361Z"
    }
   },
   "source": [
    "class ClassificationResult(StrEnum):\n",
    "    REAL = auto()\n",
    "    FAKE = auto()\n",
    "    DOUBTFUL = auto()\n",
    "\n",
    "class Source(BaseModel):\n",
    "    url: str\n",
    "    title: str\n",
    "    snippet: str\n",
    "    credibility_score: float = 0.5\n",
    "\n",
    "class Evidence(BaseModel):\n",
    "    claim: str\n",
    "    supporting_text: str\n",
    "    source: Source\n",
    "    relevance_score: float\n",
    "\n",
    "class VerificationState(TypedDict):\n",
    "    user_input: str\n",
    "    enhanced_query: str\n",
    "    search_results: List[Document]\n",
    "    evidence: List[Evidence]\n",
    "    classification: ClassificationResult\n",
    "    confidence: float\n",
    "    explanation: str\n",
    "    sources: List[Source]\n"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 1: Query Enhancement\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:25.097689Z",
     "start_time": "2025-11-29T16:54:25.093224Z"
    }
   },
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def enhance_query_node(state: VerificationState) -> VerificationState:\n",
    "    \"\"\"Enhance user query for better search results\"\"\"\n",
    "    prompt = f\"\"\"Given the following user query or claim, create an optimized search query \n",
    "    that will help verify the information. Extract key entities, dates, and facts.\n",
    "\n",
    "    User input: {state['user_input']}\n",
    "    \n",
    "    Return only the enhanced search query, nothing else.\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    enhanced = response.content.strip()\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"enhanced_query\": enhanced\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 2: Information Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:37.356287Z",
     "start_time": "2025-11-29T16:54:37.350568Z"
    }
   },
   "source": [
    "# Initialize search tool\n",
    "# Note: You need TAVILY_API_KEY for this to work\n",
    "# Alternative: Use DuckDuckGoSearchRun from langchain_community.tools\n",
    "\n",
    "try:\n",
    "    search_tool = TavilySearchResults(max_results=5)\n",
    "    USE_TAVILY = True\n",
    "except:\n",
    "    print(\"Tavily not available. Using alternative search.\")\n",
    "    USE_TAVILY = False\n",
    "\n",
    "def retrieve_information_node(state: VerificationState) -> VerificationState:\n",
    "    \"\"\"Retrieve information from web search\"\"\"\n",
    "    query = state.get('enhanced_query', state['user_input'])\n",
    "    \n",
    "    if USE_TAVILY:\n",
    "        results = search_tool.invoke({\"query\": query})\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=result.get('content', ''),\n",
    "                metadata={\n",
    "                    'url': result.get('url', ''),\n",
    "                    'title': result.get('title', '')\n",
    "                }\n",
    "            )\n",
    "            for result in results\n",
    "        ]\n",
    "    else:\n",
    "        # Fallback: Mock documents for demonstration\n",
    "        documents = [\n",
    "            Document(\n",
    "                page_content=\"Sample search result content. This would come from web search.\",\n",
    "                metadata={'url': 'https://example.com', 'title': 'Example Source'}\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"search_results\": documents\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 3: Evidence Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:43.879813Z",
     "start_time": "2025-11-29T16:54:43.876047Z"
    }
   },
   "source": [
    "def extract_evidence_node(state: VerificationState) -> VerificationState:\n",
    "    \"\"\"Extract relevant evidence from search results\"\"\"\n",
    "    user_claim = state['user_input']\n",
    "    documents = state['search_results']\n",
    "    \n",
    "    evidence_list = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        # Use LLM to extract relevant evidence\n",
    "        prompt = f\"\"\"Given the following claim and a source document, extract the most relevant \n",
    "        evidence that supports or contradicts the claim. Return only the relevant text snippet.\n",
    "\n",
    "        Claim: {user_claim}\n",
    "        \n",
    "        Source: {doc.page_content[:1000]}  # Limit length\n",
    "        \n",
    "        Extract relevant evidence:\"\"\"\n",
    "        \n",
    "        response = model.invoke([HumanMessage(content=prompt)])\n",
    "        evidence_text = response.content.strip()\n",
    "        \n",
    "        if evidence_text and len(evidence_text) > 20:  # Filter out empty/too short\n",
    "            source = Source(\n",
    "                url=doc.metadata.get('url', ''),\n",
    "                title=doc.metadata.get('title', 'Unknown'),\n",
    "                snippet=evidence_text[:200],\n",
    "                credibility_score=0.7  # Could be enhanced with domain-specific scoring\n",
    "            )\n",
    "            \n",
    "            evidence = Evidence(\n",
    "                claim=user_claim,\n",
    "                supporting_text=evidence_text,\n",
    "                source=source,\n",
    "                relevance_score=0.8  # Could use embeddings similarity\n",
    "            )\n",
    "            evidence_list.append(evidence)\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"evidence\": evidence_list\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 4: Classification (Hugging Face + LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:07.901909Z",
     "start_time": "2025-11-29T16:54:05.372996Z"
    }
   },
   "source": [
    "# Initialize Hugging Face classifier\n",
    "# Using zero-shot classification model\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "def classify_node(state: VerificationState) -> VerificationState:\n",
    "    \"\"\"Classify claim as real, fake, or doubtful\"\"\"\n",
    "    user_claim = state['user_input']\n",
    "    evidence_texts = [e.supporting_text for e in state['evidence']]\n",
    "    \n",
    "    if not evidence_texts:\n",
    "        return {\n",
    "            **state,\n",
    "            \"classification\": ClassificationResult.DOUBTFUL,\n",
    "            \"confidence\": 0.3,\n",
    "            \"explanation\": \"Insufficient evidence found to verify the claim.\"\n",
    "        }\n",
    "    \n",
    "    # Combine evidence\n",
    "    combined_evidence = \"\\n\\n\".join(evidence_texts[:3])  # Use top 3\n",
    "    \n",
    "    # Use LLM for classification (more nuanced than simple classifier)\n",
    "    prompt = f\"\"\"You are a fact-checker. Analyze the following claim against the provided evidence \n",
    "    and classify it as one of: REAL, FAKE, or DOUBTFUL.\n",
    "    \n",
    "    Claim: {user_claim}\n",
    "    \n",
    "    Evidence:\n",
    "    {combined_evidence}\n",
    "    \n",
    "    Respond in this exact format:\n",
    "    CLASSIFICATION: [REAL/FAKE/DOUBTFUL]\n",
    "    CONFIDENCE: [0.0-1.0]\n",
    "    REASONING: [brief explanation]\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    result_text = response.content\n",
    "    \n",
    "    # Parse response\n",
    "    classification = ClassificationResult.DOUBTFUL\n",
    "    confidence = 0.5\n",
    "    reasoning = result_text\n",
    "    \n",
    "    if \"REAL\" in result_text.upper():\n",
    "        classification = ClassificationResult.REAL\n",
    "    elif \"FAKE\" in result_text.upper():\n",
    "        classification = ClassificationResult.FAKE\n",
    "    \n",
    "    # Extract confidence if mentioned\n",
    "    import re\n",
    "    conf_match = re.search(r'CONFIDENCE:\\s*([0-9.]+)', result_text, re.IGNORECASE)\n",
    "    if conf_match:\n",
    "        confidence = float(conf_match.group(1))\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"classification\": classification,\n",
    "        \"confidence\": confidence,\n",
    "        \"explanation\": reasoning,\n",
    "        \"sources\": [e.source for e in state['evidence']]\n",
    "    }\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node 5: Explanation Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:07.919770Z",
     "start_time": "2025-11-29T16:54:07.915684Z"
    }
   },
   "source": [
    "def generate_explanation_node(state: VerificationState) -> VerificationState:\n",
    "    \"\"\"Generate human-readable explanation with source citations\"\"\"\n",
    "    claim = state['user_input']\n",
    "    classification = state['classification']\n",
    "    confidence = state['confidence']\n",
    "    sources = state['sources']\n",
    "    evidence = state['evidence']\n",
    "    \n",
    "    sources_text = \"\\n\".join([\n",
    "        f\"- {s.title} ({s.url})\" for s in sources[:5]\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"Generate a clear, concise explanation for the fact-checking result. \n",
    "    Include specific evidence and cite sources.\n",
    "\n",
    "    Claim: {claim}\n",
    "    Classification: {classification.value}\n",
    "    Confidence: {confidence:.2f}\n",
    "    \n",
    "    Sources:\n",
    "    {sources_text}\n",
    "    \n",
    "    Evidence snippets:\n",
    "    {chr(10).join([e.supporting_text[:200] for e in evidence[:3]])}\n",
    "    \n",
    "    Write a 2-3 sentence explanation that:\n",
    "    1. States the classification clearly\n",
    "    2. Provides key evidence\n",
    "    3. Cites the sources\n",
    "    4. Explains the confidence level\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"explanation\": response.content\n",
    "    }\n"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Routing\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:07.926006Z",
     "start_time": "2025-11-29T16:54:07.923735Z"
    }
   },
   "source": [
    "def should_retry_search(state: VerificationState) -> Literal[\"retrieve_information\", \"classify\"]:\n",
    "    \"\"\"Decide if we need more sources\"\"\"\n",
    "    # If we have no evidence or very low confidence, retry search\n",
    "    if len(state.get('evidence', [])) == 0:\n",
    "        return \"retrieve_information\"\n",
    "    if state.get('confidence', 0) < 0.4 and len(state.get('evidence', [])) < 3:\n",
    "        return \"retrieve_information\"\n",
    "    return \"classify\"\n"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:54:07.937976Z",
     "start_time": "2025-11-29T16:54:07.929076Z"
    }
   },
   "source": [
    "builder = StateGraph(VerificationState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"enhance_query\", enhance_query_node)\n",
    "builder.add_node(\"retrieve_information\", retrieve_information_node)\n",
    "builder.add_node(\"extract_evidence\", extract_evidence_node)\n",
    "builder.add_node(\"classify\", classify_node)\n",
    "builder.add_node(\"generate_explanation\", generate_explanation_node)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"enhance_query\")\n",
    "builder.add_edge(\"enhance_query\", \"retrieve_information\")\n",
    "builder.add_edge(\"retrieve_information\", \"extract_evidence\")\n",
    "builder.add_conditional_edges(\n",
    "    \"extract_evidence\",\n",
    "    should_retry_search,\n",
    "    {\n",
    "        \"retrieve_information\": \"retrieve_information\",\n",
    "        \"classify\": \"classify\"\n",
    "    }\n",
    ")\n",
    "builder.add_edge(\"classify\", \"generate_explanation\")\n",
    "builder.add_edge(\"generate_explanation\", END)\n",
    "\n",
    "# Compile\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T16:58:17.626197Z",
     "start_time": "2025-11-29T16:58:17.233665Z"
    }
   },
   "source": [
    "# Example usage\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "initial_state = VerificationState(\n",
    "    user_input=\"LeBron James scored 50 points in the 2024 NBA Finals Game 7\",\n",
    "    enhanced_query=\"\",\n",
    "    search_results=[],\n",
    "    evidence=[],\n",
    "    classification=ClassificationResult.DOUBTFUL,\n",
    "    confidence=0.0,\n",
    "    explanation=\"\",\n",
    "    sources=[]\n",
    ")\n",
    "\n",
    "result = graph.invoke(initial_state, config)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"VERIFICATION RESULT\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nClaim: {result['user_input']}\")\n",
    "print(f\"\\nClassification: {result['classification'].value}\")\n",
    "print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "print(f\"\\nExplanation:\\n{result['explanation']}\")\n",
    "print(f\"\\nSources ({len(result['sources'])}):\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"  {i}. {source.title}\")\n",
    "    print(f\"     {source.url}\")\n"
   ],
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[72], line 15\u001B[0m\n\u001B[1;32m      2\u001B[0m config \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfigurable\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthread_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m\"\u001B[39m}}\n\u001B[1;32m      4\u001B[0m initial_state \u001B[38;5;241m=\u001B[39m VerificationState(\n\u001B[1;32m      5\u001B[0m     user_input\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLeBron James scored 50 points in the 2024 NBA Finals Game 7\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     enhanced_query\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     sources\u001B[38;5;241m=\u001B[39m[]\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m=\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m60\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVERIFICATION RESULT\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/main.py:3085\u001B[0m, in \u001B[0;36mPregel.invoke\u001B[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001B[0m\n\u001B[1;32m   3082\u001B[0m chunks: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m|\u001B[39m Any] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m   3083\u001B[0m interrupts: \u001B[38;5;28mlist\u001B[39m[Interrupt] \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m-> 3085\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream(\n\u001B[1;32m   3086\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m   3087\u001B[0m     config,\n\u001B[1;32m   3088\u001B[0m     context\u001B[38;5;241m=\u001B[39mcontext,\n\u001B[1;32m   3089\u001B[0m     stream_mode\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdates\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   3090\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   3091\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m stream_mode,\n\u001B[1;32m   3092\u001B[0m     print_mode\u001B[38;5;241m=\u001B[39mprint_mode,\n\u001B[1;32m   3093\u001B[0m     output_keys\u001B[38;5;241m=\u001B[39moutput_keys,\n\u001B[1;32m   3094\u001B[0m     interrupt_before\u001B[38;5;241m=\u001B[39minterrupt_before,\n\u001B[1;32m   3095\u001B[0m     interrupt_after\u001B[38;5;241m=\u001B[39minterrupt_after,\n\u001B[1;32m   3096\u001B[0m     durability\u001B[38;5;241m=\u001B[39mdurability,\n\u001B[1;32m   3097\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   3098\u001B[0m ):\n\u001B[1;32m   3099\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stream_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3100\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(chunk) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/main.py:2674\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001B[0m\n\u001B[1;32m   2672\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m task \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mmatch_cached_writes():\n\u001B[1;32m   2673\u001B[0m     loop\u001B[38;5;241m.\u001B[39moutput_writes(task\u001B[38;5;241m.\u001B[39mid, task\u001B[38;5;241m.\u001B[39mwrites, cached\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m-> 2674\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m runner\u001B[38;5;241m.\u001B[39mtick(\n\u001B[1;32m   2675\u001B[0m     [t \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m loop\u001B[38;5;241m.\u001B[39mtasks\u001B[38;5;241m.\u001B[39mvalues() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mwrites],\n\u001B[1;32m   2676\u001B[0m     timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep_timeout,\n\u001B[1;32m   2677\u001B[0m     get_waiter\u001B[38;5;241m=\u001B[39mget_waiter,\n\u001B[1;32m   2678\u001B[0m     schedule_task\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39maccept_push,\n\u001B[1;32m   2679\u001B[0m ):\n\u001B[1;32m   2680\u001B[0m     \u001B[38;5;66;03m# emit output\u001B[39;00m\n\u001B[1;32m   2681\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _output(\n\u001B[1;32m   2682\u001B[0m         stream_mode, print_mode, subgraphs, stream\u001B[38;5;241m.\u001B[39mget, queue\u001B[38;5;241m.\u001B[39mEmpty\n\u001B[1;32m   2683\u001B[0m     )\n\u001B[1;32m   2684\u001B[0m loop\u001B[38;5;241m.\u001B[39mafter_tick()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/_runner.py:162\u001B[0m, in \u001B[0;36mPregelRunner.tick\u001B[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001B[0m\n\u001B[1;32m    160\u001B[0m t \u001B[38;5;241m=\u001B[39m tasks[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    161\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 162\u001B[0m     \u001B[43mrun_with_retry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfigurable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m            \u001B[49m\u001B[43mCONFIG_KEY_CALL\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpartial\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m                \u001B[49m\u001B[43m_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m                \u001B[49m\u001B[43mweakref\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[43m                \u001B[49m\u001B[43mretry_policy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretry_policy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[43m                \u001B[49m\u001B[43mfutures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mweakref\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mref\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfutures\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    171\u001B[0m \u001B[43m                \u001B[49m\u001B[43mschedule_task\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschedule_task\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    172\u001B[0m \u001B[43m                \u001B[49m\u001B[43msubmit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msubmit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    173\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    174\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    176\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommit(t, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/_retry.py:42\u001B[0m, in \u001B[0;36mrun_with_retry\u001B[0;34m(task, retry_policy, configurable)\u001B[0m\n\u001B[1;32m     40\u001B[0m     task\u001B[38;5;241m.\u001B[39mwrites\u001B[38;5;241m.\u001B[39mclear()\n\u001B[1;32m     41\u001B[0m     \u001B[38;5;66;03m# run the task\u001B[39;00m\n\u001B[0;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ParentCommand \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     44\u001B[0m     ns: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/_internal/_runnable.py:657\u001B[0m, in \u001B[0;36mRunnableSeq.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    655\u001B[0m     \u001B[38;5;66;03m# run in context\u001B[39;00m\n\u001B[1;32m    656\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m set_config_context(config, run) \u001B[38;5;28;01mas\u001B[39;00m context:\n\u001B[0;32m--> 657\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    658\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m step\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/_internal/_runnable.py:401\u001B[0m, in \u001B[0;36mRunnableCallable.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    399\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(ret)\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrecurse \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(ret, Runnable):\n\u001B[1;32m    403\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ret\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m, config)\n",
      "Cell \u001B[0;32mIn[68], line 12\u001B[0m, in \u001B[0;36menhance_query_node\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Enhance user query for better search results\"\"\"\u001B[39;00m\n\u001B[1;32m      5\u001B[0m prompt \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mGiven the following user query or claim, create an optimized search query \u001B[39m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124mthat will help verify the information. Extract key entities, dates, and facts.\u001B[39m\n\u001B[1;32m      7\u001B[0m \n\u001B[1;32m      8\u001B[0m \u001B[38;5;124mUser input: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstate[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124muser_input\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124m\u001B[39m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124mReturn only the enhanced search query, nothing else.\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 12\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mHumanMessage\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m enhanced \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mcontent\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstate,\n\u001B[1;32m     17\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menhanced_query\u001B[39m\u001B[38;5;124m\"\u001B[39m: enhanced\n\u001B[1;32m     18\u001B[0m }\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:395\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    391\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[1;32m    392\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[1;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m--> 395\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[1;32m    405\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:1025\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m   1017\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m   1018\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m   1023\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m   1024\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m-> 1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_messages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:842\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    839\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[1;32m    840\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    841\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[0;32m--> 842\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_with_cache\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m                \u001B[49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    848\u001B[0m         )\n\u001B[1;32m    849\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    850\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/language_models/chat_models.py:1091\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1089\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[1;32m   1090\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m-> 1091\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1092\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m   1093\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1094\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1095\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py:1213\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1211\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttp_response\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1212\u001B[0m         e\u001B[38;5;241m.\u001B[39mresponse \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mhttp_response  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m-> 1213\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m   1214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1215\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minclude_response_headers\n\u001B[1;32m   1216\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m raw_response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1217\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(raw_response, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1218\u001B[0m ):\n\u001B[1;32m   1219\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_openai/chat_models/base.py:1208\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[0;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1201\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m _construct_lc_result_from_responses_api(\n\u001B[1;32m   1202\u001B[0m             response,\n\u001B[1;32m   1203\u001B[0m             schema\u001B[38;5;241m=\u001B[39moriginal_schema_obj,\n\u001B[1;32m   1204\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mgeneration_info,\n\u001B[1;32m   1205\u001B[0m             output_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_version,\n\u001B[1;32m   1206\u001B[0m         )\n\u001B[1;32m   1207\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1208\u001B[0m         raw_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwith_raw_response\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1209\u001B[0m         response \u001B[38;5;241m=\u001B[39m raw_response\u001B[38;5;241m.\u001B[39mparse()\n\u001B[1;32m   1210\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_legacy_response.py:364\u001B[0m, in \u001B[0;36mto_raw_response_wrapper.<locals>.wrapped\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    360\u001B[0m extra_headers[RAW_RESPONSE_HEADER] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    362\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextra_headers\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m extra_headers\n\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cast(LegacyAPIResponse[R], \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_utils/_utils.py:286\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    284\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    285\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 286\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/resources/chat/completions/completions.py:1189\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m   1142\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m   1143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m   1144\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1186\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m not_given,\n\u001B[1;32m   1187\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[1;32m   1188\u001B[0m     validate_response_format(response_format)\n\u001B[0;32m-> 1189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1190\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1191\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1192\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m   1193\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1194\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1196\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1197\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1198\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1199\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1200\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1202\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1203\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1204\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1205\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1207\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_key\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_key\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt_cache_retention\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt_cache_retention\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msafety_identifier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msafety_identifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1215\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1216\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1223\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1224\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1225\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1226\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mverbosity\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbosity\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1227\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1228\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1229\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[1;32m   1230\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[1;32m   1231\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1232\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1233\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1234\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m   1235\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1236\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1238\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1239\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1259\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1245\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1246\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1247\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1254\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1255\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1256\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1257\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1258\u001B[0m     )\n\u001B[0;32m-> 1259\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/openai/_base_client.py:1047\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1044\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[1;32m   1046\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1047\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1049\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m   1051\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: your-ope************here. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-Specific Enhancements (Example: Sports)\n",
    "\n",
    "For sports domain, you can add:\n",
    "1. Official API integration (NBA Stats API, etc.)\n",
    "2. Domain-specific source whitelist\n",
    "3. Structured data validation (scores, dates, stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Domain-specific source scoring\n",
    "def score_source_credibility(source: Source, domain: str = \"sports\") -> float:\n",
    "    \"\"\"Score source credibility based on domain\"\"\"\n",
    "    trusted_domains = {\n",
    "        \"sports\": [\n",
    "            \"nba.com\", \"espn.com\", \"nfl.com\", \"mlb.com\",\n",
    "            \"basketball-reference.com\", \"pro-football-reference.com\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    url_lower = source.url.lower()\n",
    "    base_score = 0.5\n",
    "    \n",
    "    # Boost score for trusted domains\n",
    "    for trusted in trusted_domains.get(domain, []):\n",
    "        if trusted in url_lower:\n",
    "            base_score = 0.9\n",
    "            break\n",
    "    \n",
    "    return base_score\n",
    "\n",
    "# Example: Structured data validation for sports claims\n",
    "def validate_sports_claim(claim: str) -> dict:\n",
    "    \"\"\"Extract structured information from sports claim\"\"\"\n",
    "    # Use LLM to extract: player, team, stat, date, game\n",
    "    prompt = f\"\"\"Extract structured information from this sports claim:\n",
    "    {claim}\n",
    "    \n",
    "    Return JSON with: player, team, stat_type, stat_value, date, game_context\"\"\"\n",
    "    \n",
    "    response = model.invoke([HumanMessage(content=prompt)])\n",
    "    # Parse JSON response\n",
    "    # Then validate against official API\n",
    "    \n",
    "    return {\"extracted\": \"data\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps & Improvements\n",
    "\n",
    "1. **Fine-tune Classification Model**: Train on fact-checking datasets (FEVER, PolitiFact)\n",
    "2. **Multi-query Strategy**: Generate multiple search queries for better coverage\n",
    "3. **Temporal Verification**: Check if information is outdated\n",
    "4. **Claim Decomposition**: Break complex claims into verifiable sub-claims\n",
    "5. **Source Aggregation**: Weighted voting from multiple sources\n",
    "6. **Confidence Calibration**: Improve confidence score accuracy\n",
    "7. **Domain APIs**: Integrate official APIs for structured data validation\n",
    "8. **Caching**: Cache verification results for repeated queries\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
