{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Information Verifier System\n",
        "\n",
        "This notebook implements an information verification system using:\n",
        "- **LangGraph**: Workflow orchestration\n",
        "- **LangChain**: LLM integration and tools\n",
        "- **Hugging Face**: Classification models\n",
        "\n",
        "## Architecture\n",
        "\n",
        "1. User input â†’ Query enhancement\n",
        "2. Information retrieval (web search + domain APIs)\n",
        "3. Evidence extraction and analysis\n",
        "4. Classification (real/fake/doubtful)\n",
        "5. Explanation generation with sources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install langchain langchain-openai langgraph langchain-community langchain-tavily-python transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import TypedDict, Literal, List, Annotated\n",
        "from pydantic import BaseModel\n",
        "from enum import StrEnum, auto\n",
        "\n",
        "# LangChain & LangGraph\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langgraph.graph import StateGraph, START, END, MessagesState\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import pipeline\n",
        "\n",
        "# For domain-specific APIs (example: sports)\n",
        "import requests\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup API keys (adjust based on your environment)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
        "# os.environ[\"TAVILY_API_KEY\"] = \"your-key-here\"  # Get from https://tavily.com\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## State Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ClassificationResult(StrEnum):\n",
        "    REAL = auto()\n",
        "    FAKE = auto()\n",
        "    DOUBTFUL = auto()\n",
        "\n",
        "class Source(BaseModel):\n",
        "    url: str\n",
        "    title: str\n",
        "    snippet: str\n",
        "    credibility_score: float = 0.5\n",
        "\n",
        "class Evidence(BaseModel):\n",
        "    claim: str\n",
        "    supporting_text: str\n",
        "    source: Source\n",
        "    relevance_score: float\n",
        "\n",
        "class VerificationState(TypedDict):\n",
        "    user_input: str\n",
        "    enhanced_query: str\n",
        "    search_results: List[Document]\n",
        "    evidence: List[Evidence]\n",
        "    classification: ClassificationResult\n",
        "    confidence: float\n",
        "    explanation: str\n",
        "    sources: List[Source]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node 1: Query Enhancement\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def enhance_query_node(state: VerificationState) -> VerificationState:\n",
        "    \"\"\"Enhance user query for better search results\"\"\"\n",
        "    prompt = f\"\"\"Given the following user query or claim, create an optimized search query \n",
        "    that will help verify the information. Extract key entities, dates, and facts.\n",
        "\n",
        "    User input: {state['user_input']}\n",
        "    \n",
        "    Return only the enhanced search query, nothing else.\"\"\"\n",
        "    \n",
        "    response = model.invoke([HumanMessage(content=prompt)])\n",
        "    enhanced = response.content.strip()\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"enhanced_query\": enhanced\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node 2: Information Retrieval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize search tool\n",
        "# Note: You need TAVILY_API_KEY for this to work\n",
        "# Alternative: Use DuckDuckGoSearchRun from langchain_community.tools\n",
        "\n",
        "try:\n",
        "    search_tool = TavilySearchResults(max_results=5)\n",
        "    USE_TAVILY = True\n",
        "except:\n",
        "    print(\"Tavily not available. Using alternative search.\")\n",
        "    USE_TAVILY = False\n",
        "\n",
        "def retrieve_information_node(state: VerificationState) -> VerificationState:\n",
        "    \"\"\"Retrieve information from web search\"\"\"\n",
        "    query = state.get('enhanced_query', state['user_input'])\n",
        "    \n",
        "    if USE_TAVILY:\n",
        "        results = search_tool.invoke({\"query\": query})\n",
        "        documents = [\n",
        "            Document(\n",
        "                page_content=result.get('content', ''),\n",
        "                metadata={\n",
        "                    'url': result.get('url', ''),\n",
        "                    'title': result.get('title', '')\n",
        "                }\n",
        "            )\n",
        "            for result in results\n",
        "        ]\n",
        "    else:\n",
        "        # Fallback: Mock documents for demonstration\n",
        "        documents = [\n",
        "            Document(\n",
        "                page_content=\"Sample search result content. This would come from web search.\",\n",
        "                metadata={'url': 'https://example.com', 'title': 'Example Source'}\n",
        "            )\n",
        "        ]\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"search_results\": documents\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node 3: Evidence Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_evidence_node(state: VerificationState) -> VerificationState:\n",
        "    \"\"\"Extract relevant evidence from search results\"\"\"\n",
        "    user_claim = state['user_input']\n",
        "    documents = state['search_results']\n",
        "    \n",
        "    evidence_list = []\n",
        "    \n",
        "    for doc in documents:\n",
        "        # Use LLM to extract relevant evidence\n",
        "        prompt = f\"\"\"Given the following claim and a source document, extract the most relevant \n",
        "        evidence that supports or contradicts the claim. Return only the relevant text snippet.\n",
        "\n",
        "        Claim: {user_claim}\n",
        "        \n",
        "        Source: {doc.page_content[:1000]}  # Limit length\n",
        "        \n",
        "        Extract relevant evidence:\"\"\"\n",
        "        \n",
        "        response = model.invoke([HumanMessage(content=prompt)])\n",
        "        evidence_text = response.content.strip()\n",
        "        \n",
        "        if evidence_text and len(evidence_text) > 20:  # Filter out empty/too short\n",
        "            source = Source(\n",
        "                url=doc.metadata.get('url', ''),\n",
        "                title=doc.metadata.get('title', 'Unknown'),\n",
        "                snippet=evidence_text[:200],\n",
        "                credibility_score=0.7  # Could be enhanced with domain-specific scoring\n",
        "            )\n",
        "            \n",
        "            evidence = Evidence(\n",
        "                claim=user_claim,\n",
        "                supporting_text=evidence_text,\n",
        "                source=source,\n",
        "                relevance_score=0.8  # Could use embeddings similarity\n",
        "            )\n",
        "            evidence_list.append(evidence)\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"evidence\": evidence_list\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node 4: Classification (Hugging Face + LLM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Hugging Face classifier\n",
        "# Using zero-shot classification model\n",
        "classifier = pipeline(\n",
        "    \"zero-shot-classification\",\n",
        "    model=\"facebook/bart-large-mnli\"\n",
        ")\n",
        "\n",
        "def classify_node(state: VerificationState) -> VerificationState:\n",
        "    \"\"\"Classify claim as real, fake, or doubtful\"\"\"\n",
        "    user_claim = state['user_input']\n",
        "    evidence_texts = [e.supporting_text for e in state['evidence']]\n",
        "    \n",
        "    if not evidence_texts:\n",
        "        return {\n",
        "            **state,\n",
        "            \"classification\": ClassificationResult.DOUBTFUL,\n",
        "            \"confidence\": 0.3,\n",
        "            \"explanation\": \"Insufficient evidence found to verify the claim.\"\n",
        "        }\n",
        "    \n",
        "    # Combine evidence\n",
        "    combined_evidence = \"\\n\\n\".join(evidence_texts[:3])  # Use top 3\n",
        "    \n",
        "    # Use LLM for classification (more nuanced than simple classifier)\n",
        "    prompt = f\"\"\"You are a fact-checker. Analyze the following claim against the provided evidence \n",
        "    and classify it as one of: REAL, FAKE, or DOUBTFUL.\n",
        "    \n",
        "    Claim: {user_claim}\n",
        "    \n",
        "    Evidence:\n",
        "    {combined_evidence}\n",
        "    \n",
        "    Respond in this exact format:\n",
        "    CLASSIFICATION: [REAL/FAKE/DOUBTFUL]\n",
        "    CONFIDENCE: [0.0-1.0]\n",
        "    REASONING: [brief explanation]\"\"\"\n",
        "    \n",
        "    response = model.invoke([HumanMessage(content=prompt)])\n",
        "    result_text = response.content\n",
        "    \n",
        "    # Parse response\n",
        "    classification = ClassificationResult.DOUBTFUL\n",
        "    confidence = 0.5\n",
        "    reasoning = result_text\n",
        "    \n",
        "    if \"REAL\" in result_text.upper():\n",
        "        classification = ClassificationResult.REAL\n",
        "    elif \"FAKE\" in result_text.upper():\n",
        "        classification = ClassificationResult.FAKE\n",
        "    \n",
        "    # Extract confidence if mentioned\n",
        "    import re\n",
        "    conf_match = re.search(r'CONFIDENCE:\\s*([0-9.]+)', result_text, re.IGNORECASE)\n",
        "    if conf_match:\n",
        "        confidence = float(conf_match.group(1))\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"classification\": classification,\n",
        "        \"confidence\": confidence,\n",
        "        \"explanation\": reasoning,\n",
        "        \"sources\": [e.source for e in state['evidence']]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Node 5: Explanation Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_explanation_node(state: VerificationState) -> VerificationState:\n",
        "    \"\"\"Generate human-readable explanation with source citations\"\"\"\n",
        "    claim = state['user_input']\n",
        "    classification = state['classification']\n",
        "    confidence = state['confidence']\n",
        "    sources = state['sources']\n",
        "    evidence = state['evidence']\n",
        "    \n",
        "    sources_text = \"\\n\".join([\n",
        "        f\"- {s.title} ({s.url})\" for s in sources[:5]\n",
        "    ])\n",
        "    \n",
        "    prompt = f\"\"\"Generate a clear, concise explanation for the fact-checking result. \n",
        "    Include specific evidence and cite sources.\n",
        "\n",
        "    Claim: {claim}\n",
        "    Classification: {classification.value}\n",
        "    Confidence: {confidence:.2f}\n",
        "    \n",
        "    Sources:\n",
        "    {sources_text}\n",
        "    \n",
        "    Evidence snippets:\n",
        "    {chr(10).join([e.supporting_text[:200] for e in evidence[:3]])}\n",
        "    \n",
        "    Write a 2-3 sentence explanation that:\n",
        "    1. States the classification clearly\n",
        "    2. Provides key evidence\n",
        "    3. Cites the sources\n",
        "    4. Explains the confidence level\"\"\"\n",
        "    \n",
        "    response = model.invoke([HumanMessage(content=prompt)])\n",
        "    \n",
        "    return {\n",
        "        **state,\n",
        "        \"explanation\": response.content\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional Routing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def should_retry_search(state: VerificationState) -> Literal[\"retrieve_information\", \"classify\"]:\n",
        "    \"\"\"Decide if we need more sources\"\"\"\n",
        "    # If we have no evidence or very low confidence, retry search\n",
        "    if len(state.get('evidence', [])) == 0:\n",
        "        return \"retrieve_information\"\n",
        "    if state.get('confidence', 0) < 0.4 and len(state.get('evidence', [])) < 3:\n",
        "        return \"retrieve_information\"\n",
        "    return \"classify\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "builder = StateGraph(VerificationState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"enhance_query\", enhance_query_node)\n",
        "builder.add_node(\"retrieve_information\", retrieve_information_node)\n",
        "builder.add_node(\"extract_evidence\", extract_evidence_node)\n",
        "builder.add_node(\"classify\", classify_node)\n",
        "builder.add_node(\"generate_explanation\", generate_explanation_node)\n",
        "\n",
        "# Add edges\n",
        "builder.add_edge(START, \"enhance_query\")\n",
        "builder.add_edge(\"enhance_query\", \"retrieve_information\")\n",
        "builder.add_edge(\"retrieve_information\", \"extract_evidence\")\n",
        "builder.add_conditional_edges(\n",
        "    \"extract_evidence\",\n",
        "    should_retry_search,\n",
        "    {\n",
        "        \"retrieve_information\": \"retrieve_information\",\n",
        "        \"classify\": \"classify\"\n",
        "    }\n",
        ")\n",
        "builder.add_edge(\"classify\", \"generate_explanation\")\n",
        "builder.add_edge(\"generate_explanation\", END)\n",
        "\n",
        "# Compile\n",
        "memory = MemorySaver()\n",
        "graph = builder.compile(checkpointer=memory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "initial_state = VerificationState(\n",
        "    user_input=\"LeBron James scored 50 points in the 2024 NBA Finals Game 7\",\n",
        "    enhanced_query=\"\",\n",
        "    search_results=[],\n",
        "    evidence=[],\n",
        "    classification=ClassificationResult.DOUBTFUL,\n",
        "    confidence=0.0,\n",
        "    explanation=\"\",\n",
        "    sources=[]\n",
        ")\n",
        "\n",
        "result = graph.invoke(initial_state, config)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"VERIFICATION RESULT\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nClaim: {result['user_input']}\")\n",
        "print(f\"\\nClassification: {result['classification'].value}\")\n",
        "print(f\"Confidence: {result['confidence']:.2%}\")\n",
        "print(f\"\\nExplanation:\\n{result['explanation']}\")\n",
        "print(f\"\\nSources ({len(result['sources'])}):\")\n",
        "for i, source in enumerate(result['sources'], 1):\n",
        "    print(f\"  {i}. {source.title}\")\n",
        "    print(f\"     {source.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain-Specific Enhancements (Example: Sports)\n",
        "\n",
        "For sports domain, you can add:\n",
        "1. Official API integration (NBA Stats API, etc.)\n",
        "2. Domain-specific source whitelist\n",
        "3. Structured data validation (scores, dates, stats)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Domain-specific source scoring\n",
        "def score_source_credibility(source: Source, domain: str = \"sports\") -> float:\n",
        "    \"\"\"Score source credibility based on domain\"\"\"\n",
        "    trusted_domains = {\n",
        "        \"sports\": [\n",
        "            \"nba.com\", \"espn.com\", \"nfl.com\", \"mlb.com\",\n",
        "            \"basketball-reference.com\", \"pro-football-reference.com\"\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    url_lower = source.url.lower()\n",
        "    base_score = 0.5\n",
        "    \n",
        "    # Boost score for trusted domains\n",
        "    for trusted in trusted_domains.get(domain, []):\n",
        "        if trusted in url_lower:\n",
        "            base_score = 0.9\n",
        "            break\n",
        "    \n",
        "    return base_score\n",
        "\n",
        "# Example: Structured data validation for sports claims\n",
        "def validate_sports_claim(claim: str) -> dict:\n",
        "    \"\"\"Extract structured information from sports claim\"\"\"\n",
        "    # Use LLM to extract: player, team, stat, date, game\n",
        "    prompt = f\"\"\"Extract structured information from this sports claim:\n",
        "    {claim}\n",
        "    \n",
        "    Return JSON with: player, team, stat_type, stat_value, date, game_context\"\"\"\n",
        "    \n",
        "    response = model.invoke([HumanMessage(content=prompt)])\n",
        "    # Parse JSON response\n",
        "    # Then validate against official API\n",
        "    \n",
        "    return {\"extracted\": \"data\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps & Improvements\n",
        "\n",
        "1. **Fine-tune Classification Model**: Train on fact-checking datasets (FEVER, PolitiFact)\n",
        "2. **Multi-query Strategy**: Generate multiple search queries for better coverage\n",
        "3. **Temporal Verification**: Check if information is outdated\n",
        "4. **Claim Decomposition**: Break complex claims into verifiable sub-claims\n",
        "5. **Source Aggregation**: Weighted voting from multiple sources\n",
        "6. **Confidence Calibration**: Improve confidence score accuracy\n",
        "7. **Domain APIs**: Integrate official APIs for structured data validation\n",
        "8. **Caching**: Cache verification results for repeated queries\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
